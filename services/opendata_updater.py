# services/opendata_updater.py
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ opendata_full.json —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏.
–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–∞–∫ —Ñ–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –±–æ—Ç–∞ –∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ.
"""

import os
import json
import asyncio
import logging
from datetime import datetime, timezone
import httpx
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

API_KEY = os.getenv("NV_OPENDATA_API_KEY", "")
BASE_URL = "https://data.n-vartovsk.ru/api/v1"
DATA_FILE = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "opendata_full.json")
UPDATE_INTERVAL = 86400  # 24 —á–∞—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

# –í—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
DATASETS = {
    "listoumd": "8603032896-listoumd",
    "agstruct": "8603032896-agstruct",
    "agphonedir": "8603032896-agphonedir",
    "uchgkhservices": "8603032896-uchgkhservices",
    "tarif": "8603032896-tarif",
    "wastecollection": "8603032896-wastecollection",
    "buildlist": "8603032896-buildlist",
    "uchdou": "8603032896-uchdou",
    "uchou": "8603032896-uchou",
    "uchsport": "8603032896-uchsport",
    "uchculture": "8603032896-uchculture",
    "uchsportsection": "8603032896-uchsportsection",
    "topnameboys": "8603032896-topnameboys",
    "topnamegirls": "8603032896-topnamegirls",
    "averagesalary": "8603032896-averagesalary",
    "roadgasstationprice": "8603032896-roadgasstationprice",
    "mspsupport": "8603032896-mspsupport",
    "placespk": "8603032896-placespk",
    "placessg": "8603032896-placessg",
    "territoryplans": "8603032896-territoryplans",
}


async def _fetch_all_pages(client: httpx.AsyncClient, ds_id: str) -> list:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    all_rows = []
    page = 1
    while True:
        url = f"{BASE_URL}/{ds_id}/data?api_key={API_KEY}&ROWS=500&PAGE={page}"
        try:
            r = await client.get(url, headers={"User-Agent": "PulsGoroda/1.0"})
            if r.status_code != 200:
                break
            data = r.json()
            result = data.get("RESULT", {})
            rows = result.get("ROWS", [])
            if not rows:
                break
            all_rows.extend(rows)
            total_pages = result.get("META", {}).get("PAGE_TOTAL", 1)
            if page >= total_pages:
                break
            page += 1
        except Exception as e:
            logger.error(f"Fetch page {page} of {ds_id}: {e}")
            break
    return all_rows


async def update_opendata() -> dict:
    """–û–±–Ω–æ–≤–ª—è–µ—Ç opendata_full.json —Å–æ –≤—Å–µ–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏."""
    if not API_KEY:
        logger.warning("‚ö†Ô∏è NV_OPENDATA_API_KEY –Ω–µ –∑–∞–¥–∞–Ω, –ø—Ä–æ–ø—É—Å–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
        return {"error": "no api key"}

    logger.info("üîÑ –ù–∞—á–∏–Ω–∞—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    result = {}
    now = datetime.now(timezone.utc).isoformat()

    async with httpx.AsyncClient(timeout=30.0) as client:
        for key, ds_id in DATASETS.items():
            try:
                rows = await _fetch_all_pages(client, ds_id)
                result[key] = {"rows": rows, "meta": {"updated": now, "count": len(rows)}}
                logger.info(f"  ‚úÖ {key}: {len(rows)} –∑–∞–ø–∏—Å–µ–π")
            except Exception as e:
                logger.error(f"  ‚ùå {key}: {e}")
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                if os.path.exists(DATA_FILE):
                    try:
                        with open(DATA_FILE, "r", encoding="utf-8") as f:
                            old = json.load(f)
                        if key in old:
                            result[key] = old[key]
                            logger.info(f"  ‚ôªÔ∏è {key}: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à")
                    except Exception:
                        pass

    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    result["_meta"] = {"updated_at": now, "datasets_count": len(DATASETS)}

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    try:
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False)
        logger.info(f"‚úÖ opendata_full.json –æ–±–Ω–æ–≤–ª—ë–Ω ({len(result)-1} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤)")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏: {e}")

    return result


def get_last_update() -> str | None:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è."""
    try:
        if os.path.exists(DATA_FILE):
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            return data.get("_meta", {}).get("updated_at")
    except Exception:
        pass
    return None


def needs_update() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (–ø—Ä–æ—à–ª–æ > 24—á)."""
    last = get_last_update()
    if not last:
        return True
    try:
        last_dt = datetime.fromisoformat(last)
        diff = datetime.now(timezone.utc) - last_dt
        return diff.total_seconds() > UPDATE_INTERVAL
    except Exception:
        return True


async def auto_update_loop():
    """–§–æ–Ω–æ–≤–æ–π —Ü–∏–∫–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏."""
    logger.info("üîÑ –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ opendata –∑–∞–ø—É—â–µ–Ω–æ (–∏–Ω—Ç–µ—Ä–≤–∞–ª: 24—á)")
    while True:
        try:
            if needs_update():
                await update_opendata()
            else:
                last = get_last_update()
                logger.info(f"üì¶ –î–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã (–æ–±–Ω–æ–≤–ª–µ–Ω—ã: {last})")
        except Exception as e:
            logger.error(f"Auto-update error: {e}")
        await asyncio.sleep(UPDATE_INTERVAL)
