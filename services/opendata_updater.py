# services/opendata_updater.py
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ opendata_full.json —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏.
–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–∞–∫ —Ñ–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –±–æ—Ç–∞ –∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ.
"""

import os
import json
import asyncio
import logging
from datetime import datetime, timezone
import httpx
from core.http_client import get_http_client
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

API_KEY = os.getenv("NV_OPENDATA_API_KEY", "")
BASE_URL = "https://data.n-vartovsk.ru/api/v1"
DATA_FILE = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "opendata_full.json")
UPDATE_INTERVAL = 86400  # 24 —á–∞—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

# –í—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
DATASETS = {
    "listoumd": "8603032896-listoumd",
    "agstruct": "8603032896-agstruct",
    "agphonedir": "8603032896-agphonedir",
    "uchgkhservices": "8603032896-uchgkhservices",
    "tarif": "8603032896-tarif",
    "wastecollection": "8603032896-wastecollection",
    "buildlist": "8603032896-buildlist",
    "uchdou": "8603032896-uchdou",
    "uchou": "8603032896-uchou",
    "uchsport": "8603032896-uchsport",
    "uchculture": "8603032896-uchculture",
    "uchsportsection": "8603032896-uchsportsection",
    "topnameboys": "8603032896-topnameboys",
    "topnamegirls": "8603032896-topnamegirls",
    "averagesalary": "8603032896-averagesalary",
    "roadgasstationprice": "8603032896-roadgasstationprice",
    "mspsupport": "8603032896-mspsupport",
    "placespk": "8603032896-placespk",
    "placessg": "8603032896-placessg",
    "territoryplans": "8603032896-territoryplans",
    # –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∏ –¥–æ—Ä–æ–≥–∏
    "busroute": "8603032896-busroute",
    "busstation": "8603032896-busstation",
    "roadgasstation": "8603032896-roadgasstation",
    "roadservice": "8603032896-roadservice",
    "roadworks": "8603032896-roadworks",
    # –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ –∫—É–ª—å—Ç—É—Ä–∞
    "uchcultureclubs": "8603032896-uchcultureclubs",
    "uchsporttrainers": "8603032896-uchsporttrainers",
    "uchoudod": "8603032896-uchoudod",
    # –°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –∏ –∑–µ–º–ª—è
    "buildpermission": "8603032896-buildpermission",
    "buildreestr": "8603032896-buildreestr",
    "landplotsreestr": "8603032896-landplotsreestr",
    # –î–æ—Å—Ç—É–ø–Ω–∞—è —Å—Ä–µ–¥–∞ –∏ –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—è
    "dostupnayasreda": "8603032896-dostupnayasreda",
    "demography": "8603032896-demography",
    "publichearing": "8603032896-publichearing",
    "stvpgmu": "8603032896-stvpgmu",
    # === –ë–Æ–î–ñ–ï–¢ ===
    "budgetbulletin": "8603032896-budgetbulletin",
    "budgetinfo": "8603032896-budgetinfo",
    "budgetreport": "8603032896-budgetreport",
    # === –ú–£–ù–ò–¶–ò–ü–ê–õ–¨–ù–´–ï –ö–û–ù–¢–†–ê–ö–¢–´ (agreements) ===
    "agreementsdai": "8603032896-agreementsdai",       # –î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã –∏–º—É—â–µ—Å—Ç–≤–∞
    "agreementsdkr": "8603032896-agreementsdkr",       # –î–æ–≥–æ–≤–æ—Ä—ã –∫–∞–ø—Ä–µ–º–æ–Ω—Ç–∞
    "agreementsek": "8603032896-agreementsek",         # –î–æ–≥–æ–≤–æ—Ä—ã —ç–Ω–µ—Ä–≥–æ—Å–µ—Ä–≤–∏—Å
    "agreementsgchp": "8603032896-agreementsgchp",     # –î–æ–≥–æ–≤–æ—Ä—ã –ì–ß–ü
    "agreementsiip": "8603032896-agreementsiip",       # –î–æ–≥–æ–≤–æ—Ä—ã –∏–Ω–≤–µ—Å—Ç–ø—Ä–æ–µ–∫—Ç—ã
    "agreementsik": "8603032896-agreementsik",         # –î–æ–≥–æ–≤–æ—Ä—ã –∏–Ω–≤–µ—Å—Ç–∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã
    "agreementskjc": "8603032896-agreementskjc",       # –î–æ–≥–æ–≤–æ—Ä—ã –ö–ñ–¶
    "agreementsrip": "8603032896-agreementsrip",       # –î–æ–≥–æ–≤–æ—Ä—ã –†–ò–ü
    "agreementssp": "8603032896-agreementssp",         # –î–æ–≥–æ–≤–æ—Ä—ã —Å–æ—Ü–ø–∞—Ä—Ç–Ω—ë—Ä—Å—Ç–≤–æ
    "agreementszpk": "8603032896-agreementszpk",       # –î–æ–≥–æ–≤–æ—Ä—ã –ó–ü–ö
    # === –ò–ú–£–©–ï–°–¢–í–û ===
    "propertyregisterlands": "8603032896-propertyregisterlands",
    "propertyregistermovableproperty": "8603032896-propertyregistermovableproperty",
    "propertyregisterrealestate": "8603032896-propertyregisterrealestate",
    "propertyregisterstoks": "8603032896-propertyregisterstoks",
    "infoprivatization": "8603032896-infoprivatization",
    "inforent": "8603032896-inforent",
    # === –ë–ò–ó–ù–ï–° ===
    "businessevents": "8603032896-businessevents",
    "businessinfo": "8603032896-businessinfo",
    "msgsmp": "8603032896-msgsmp",
    # === –†–ï–ö–õ–ê–ú–ê –ò –°–í–Ø–ó–¨ ===
    "advertisingconstructions": "8603032896-advertisingconstructions",
    "listcommunicationequipment": "8603032896-listcommunicationequipment",
    # === –ê–†–•–ò–í –ò –î–û–ö–£–ú–ï–ù–¢–´ ===
    "archiveexpertise": "8603032896-archiveexpertise",
    "archivelistag": "8603032896-archivelistag",
    "docag": "8603032896-docag",
    "docaglink": "8603032896-docaglink",
    "docagtext": "8603032896-docagtext",
    "prglistag": "8603032896-prglistag",
    # === –ù–û–í–û–°–¢–ò –ò –§–û–¢–û ===
    "sitelenta": "8603032896-sitelenta",
    "sitenews": "8603032896-sitenews",
    "siterubrics": "8603032896-siterubrics",
    "photoreports": "8603032896-photoreports",
    # === –ü–†–û–ß–ï–ï ===
    "ogobsor": "8603032896-ogobsor",
    "otguid": "8603032896-otguid",
    "placesad": "8603032896-placesad",
}


async def _fetch_all_pages(client: httpx.AsyncClient, ds_id: str) -> list:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    all_rows = []
    page = 1
    while True:
        url = f"{BASE_URL}/{ds_id}/data?api_key={API_KEY}&ROWS=500&PAGE={page}"
        try:
            r = await client.get(url, headers={"User-Agent": "PulsGoroda/1.0"})
            if r.status_code != 200:
                break
            data = r.json()
            result = data.get("RESULT", {})
            rows = result.get("ROWS", [])
            if not rows:
                break
            all_rows.extend(rows)
            total_pages = result.get("META", {}).get("PAGE_TOTAL", 1)
            if page >= total_pages:
                break
            page += 1
        except Exception as e:
            logger.error(f"Fetch page {page} of {ds_id}: {e}")
            break
    return all_rows


async def update_opendata() -> dict:
    """–û–±–Ω–æ–≤–ª—è–µ—Ç opendata_full.json —Å–æ –≤—Å–µ–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏."""
    if not API_KEY:
        logger.warning("‚ö†Ô∏è NV_OPENDATA_API_KEY –Ω–µ –∑–∞–¥–∞–Ω, –ø—Ä–æ–ø—É—Å–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
        return {"error": "no api key"}

    logger.info("üîÑ –ù–∞—á–∏–Ω–∞—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    result = {}
    now = datetime.now(timezone.utc).isoformat()

    async with get_http_client(timeout=30.0) as client:
        for key, ds_id in DATASETS.items():
            try:
                rows = await _fetch_all_pages(client, ds_id)
                result[key] = {"rows": rows, "meta": {"updated": now, "count": len(rows)}}
                logger.info(f"  ‚úÖ {key}: {len(rows)} –∑–∞–ø–∏—Å–µ–π")
            except Exception as e:
                logger.error(f"  ‚ùå {key}: {e}")
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                if os.path.exists(DATA_FILE):
                    try:
                        with open(DATA_FILE, "r", encoding="utf-8") as f:
                            old = json.load(f)
                        if key in old:
                            result[key] = old[key]
                            logger.info(f"  ‚ôªÔ∏è {key}: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à")
                    except Exception:
                        pass

    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    result["_meta"] = {"updated_at": now, "datasets_count": len(DATASETS)}

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    try:
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False)
        logger.info(f"‚úÖ opendata_full.json –æ–±–Ω–æ–≤–ª—ë–Ω ({len(result)-1} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤)")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏: {e}")

    return result


def get_last_update() -> str | None:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è."""
    try:
        if os.path.exists(DATA_FILE):
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            return data.get("_meta", {}).get("updated_at")
    except Exception:
        pass
    return None


def needs_update() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (–ø—Ä–æ—à–ª–æ > 24—á)."""
    last = get_last_update()
    if not last:
        return True
    try:
        last_dt = datetime.fromisoformat(last)
        diff = datetime.now(timezone.utc) - last_dt
        return diff.total_seconds() > UPDATE_INTERVAL
    except Exception:
        return True


async def auto_update_loop():
    """–§–æ–Ω–æ–≤–æ–π —Ü–∏–∫–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏."""
    logger.info("üîÑ –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ opendata –∑–∞–ø—É—â–µ–Ω–æ (–∏–Ω—Ç–µ—Ä–≤–∞–ª: 24—á)")
    while True:
        try:
            if needs_update():
                await update_opendata()
            else:
                last = get_last_update()
                logger.info(f"üì¶ –î–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã (–æ–±–Ω–æ–≤–ª–µ–Ω—ã: {last})")
        except Exception as e:
            logger.error(f"Auto-update error: {e}")
        await asyncio.sleep(UPDATE_INTERVAL)
