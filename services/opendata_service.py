# services/opendata_service.py
"""–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ data.n-vartovsk.ru"""

import os
import json
import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import httpx
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

API_KEY = os.getenv("NV_OPENDATA_API_KEY", "")
BASE_URL = "https://data.n-vartovsk.ru/api/v1"
CACHE_FILE = "opendata_cache.json"
CACHE_TTL_HOURS = 24

# –ö–ª—é—á–µ–≤—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –ñ–ö–•-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
DATASETS = {
    "listoumd": {
        "id": "8603032896-listoumd",
        "name": "–£–ø—Ä–∞–≤–ª—è—é—â–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏",
        "icon": "üè¢",
        "description": "–ü–µ—Ä–µ—á–µ–Ω—å –£–ö, –¢–°–ñ, –ñ–°–ö –≥–æ—Ä–æ–¥–∞",
    },
    "agstruct": {
        "id": "8603032896-agstruct",
        "name": "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏",
        "icon": "üèõÔ∏è",
        "description": "–ü–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≥–æ—Ä–æ–¥–∞",
    },
    "agphonedir": {
        "id": "8603032896-agphonedir",
        "name": "–¢–µ–ª–µ—Ñ–æ–Ω–Ω—ã–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫",
        "icon": "üìû",
        "description": "–ö–æ–Ω—Ç–∞–∫—Ç—ã –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≥–æ—Ä–æ–¥–∞",
    },
    "uchgkhservices": {
        "id": "8603032896-uchgkhservices",
        "name": "–£—Å–ª—É–≥–∏ –ñ–ö–•",
        "icon": "üîß",
        "description": "–ü–µ—Ä–µ—á–µ–Ω—å —É—Å–ª—É–≥ –ñ–ö–•",
    },
    "tarif": {
        "id": "8603032896-tarif",
        "name": "–¢–∞—Ä–∏—Ñ—ã –ñ–ö–•",
        "icon": "üí∞",
        "description": "–¢–∞—Ä–∏—Ñ—ã –Ω–∞ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏",
    },
    "wastecollection": {
        "id": "8603032896-wastecollection",
        "name": "–í—ã–≤–æ–∑ –º—É—Å–æ—Ä–∞",
        "icon": "üóëÔ∏è",
        "description": "–ü–ª–æ—â–∞–¥–∫–∏ —Å–±–æ—Ä–∞ –æ—Ç—Ö–æ–¥–æ–≤",
    },
    "buildlist": {
        "id": "8603032896-buildlist",
        "name": "–°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ",
        "icon": "üèóÔ∏è",
        "description": "–û–±—ä–µ–∫—Ç—ã —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞",
    },
    "uchdou": {
        "id": "8603032896-uchdou",
        "name": "–î–µ—Ç—Å–∫–∏–µ —Å–∞–¥—ã",
        "icon": "üë∂",
        "description": "–î–æ—à–∫–æ–ª—å–Ω—ã–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏—è",
    },
    "uchou": {
        "id": "8603032896-uchou",
        "name": "–®–∫–æ–ª—ã",
        "icon": "üè´",
        "description": "–û–±—â–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏—è",
    },
    "uchsport": {
        "id": "8603032896-uchsport",
        "name": "–°–ø–æ—Ä—Ç–∏–≤–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã",
        "icon": "‚öΩ",
        "description": "–°–ø–æ—Ä—Ç–∏–≤–Ω—ã–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏—è",
    },
    "uchculture": {
        "id": "8603032896-uchculture",
        "name": "–£—á—Ä–µ–∂–¥–µ–Ω–∏—è –∫—É–ª—å—Ç—É—Ä—ã",
        "icon": "üé≠",
        "description": "–ö—É–ª—å—Ç—É—Ä–Ω—ã–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏—è –≥–æ—Ä–æ–¥–∞",
    },
    "placesad": {
        "id": "8603032896-placesad",
        "name": "–†–µ–∫–ª–∞–º–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏",
        "icon": "üìã",
        "description": "–ú–µ—Å—Ç–∞ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è —Ä–µ–∫–ª–∞–º—ã",
    },
}


# –ö—ç—à –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç–∏
_cache: Dict[str, Any] = {}
_cache_time: Optional[datetime] = None


def _load_cache() -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à –∏–∑ —Ñ–∞–π–ª–∞"""
    global _cache, _cache_time
    try:
        if os.path.exists(CACHE_FILE):
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            _cache_time = datetime.fromisoformat(data.get("updated_at", "2000-01-01"))
            _cache = data.get("datasets", {})
            return _cache
    except Exception as e:
        logger.error(f"Cache load error: {e}")
    return {}


def _save_cache():
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à –≤ —Ñ–∞–π–ª"""
    try:
        data = {
            "updated_at": datetime.utcnow().isoformat(),
            "datasets": _cache,
        }
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"Cache save error: {e}")


def _is_cache_fresh() -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –∫—ç—à–∞"""
    if not _cache_time:
        return False
    return (datetime.utcnow() - _cache_time) < timedelta(hours=CACHE_TTL_HOURS)


async def fetch_dataset(dataset_id: str, rows: int = 100, page: int = 1) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —á–µ—Ä–µ–∑ API"""
    if not API_KEY:
        return {"error": "NV_OPENDATA_API_KEY –Ω–µ –∑–∞–¥–∞–Ω –≤ .env"}

    url = f"{BASE_URL}/{dataset_id}/data?api_key={API_KEY}&ROWS={rows}&PAGE={page}"
    try:
        async with httpx.AsyncClient(timeout=15.0) as client:
            r = await client.get(url, headers={"User-Agent": "PulsGoroda/1.0"})
            if r.status_code == 200:
                return r.json()
            return {"error": f"HTTP {r.status_code}", "body": r.text[:200]}
    except Exception as e:
        return {"error": str(e)}


async def fetch_passport(dataset_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Å–ø–æ—Ä—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    if not API_KEY:
        return {"error": "NV_OPENDATA_API_KEY –Ω–µ –∑–∞–¥–∞–Ω"}

    url = f"{BASE_URL}/{dataset_id}/passport?api_key={API_KEY}"
    try:
        async with httpx.AsyncClient(timeout=15.0) as client:
            r = await client.get(url, headers={"User-Agent": "PulsGoroda/1.0"})
            if r.status_code == 200:
                return r.json()
            return {"error": f"HTTP {r.status_code}"}
    except Exception as e:
        return {"error": str(e)}


async def refresh_all_datasets() -> Dict[str, Any]:
    """–û–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —Ä–∞–∑ –≤ –¥–µ–Ω—å)"""
    global _cache, _cache_time
    results = {}

    for key, ds_info in DATASETS.items():
        ds_id = ds_info["id"]
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞
            data = await fetch_dataset(ds_id, rows=5, page=1)
            if "error" in data:
                results[key] = {"error": data["error"]}
                continue

            result = data.get("RESULT", {})
            meta = result.get("META", {})
            rows_sample = result.get("ROWS", [])

            summary = {
                "id": ds_id,
                "name": ds_info["name"],
                "icon": ds_info["icon"],
                "description": ds_info["description"],
                "total_rows": meta.get("ROWS_TOTAL", 0),
                "total_pages": meta.get("PAGE_TOTAL", 0),
                "sample": rows_sample[:2],  # 2 –ø—Ä–∏–º–µ—Ä–∞
                "updated_at": datetime.utcnow().isoformat(),
            }

            _cache[key] = summary
            results[key] = summary
            logger.info(f"‚úÖ {ds_info['name']}: {meta.get('ROWS_TOTAL', 0)} –∑–∞–ø–∏—Å–µ–π")

        except Exception as e:
            results[key] = {"error": str(e)}
            logger.error(f"‚ùå {ds_info['name']}: {e}")

    _cache_time = datetime.utcnow()
    _save_cache()
    return results


async def get_all_summaries() -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å—É–º–º–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º (–∏–∑ –∫—ç—à–∞ –∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å)"""
    global _cache
    if not _cache:
        _load_cache()

    if not _cache or not _is_cache_fresh():
        await refresh_all_datasets()

    return {
        "success": True,
        "updated_at": _cache_time.isoformat() if _cache_time else None,
        "datasets": _cache,
        "count": len(_cache),
    }


async def get_dataset_detail(key: str, rows: int = 20, page: int = 1) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    ds_info = DATASETS.get(key)
    if not ds_info:
        return {"error": f"–î–∞—Ç–∞—Å–µ—Ç '{key}' –Ω–µ –Ω–∞–π–¥–µ–Ω"}

    data = await fetch_dataset(ds_info["id"], rows=rows, page=page)
    if "error" in data:
        return data

    result = data.get("RESULT", {})
    return {
        "success": True,
        "name": ds_info["name"],
        "icon": ds_info["icon"],
        "meta": result.get("META", {}),
        "rows": result.get("ROWS", []),
    }


async def search_uk_by_address(address: str) -> List[Dict[str, Any]]:
    """–ù–∞–π—Ç–∏ —É–ø—Ä–∞–≤–ª—è—é—â—É—é –∫–æ–º–ø–∞–Ω–∏—é –ø–æ –∞–¥—Ä–µ—Å—É"""
    data = await fetch_dataset("8603032896-listoumd", rows=100, page=1)
    if "error" in data:
        return []

    rows = data.get("RESULT", {}).get("ROWS", [])
    results = []
    addr_lower = address.lower()

    for uk in rows:
        mkd_list = uk.get("MKD", [])
        for mkd in mkd_list:
            street = (mkd.get("STREET") or "").lower()
            buildings = mkd.get("BUILDINGS", [])
            if addr_lower in street or street in addr_lower:
                results.append({
                    "uk_name": uk.get("TITLESM") or uk.get("TITLE"),
                    "uk_full": uk.get("TITLE"),
                    "phone": uk.get("TEL"),
                    "email": uk.get("EMAIL"),
                    "address": uk.get("ADR"),
                    "url": uk.get("URL"),
                    "fio": uk.get("FIO"),
                    "street": mkd.get("STREET"),
                    "buildings": buildings,
                    "cnt": uk.get("CNT"),
                })
                break

    return results
